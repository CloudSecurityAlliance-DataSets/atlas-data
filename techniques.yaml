---

- id: MLT0000
  object-type: technique
  name: Acquire OSINT Information
  description: |
    Adversaries may leverage publicly available information, or Open Source Intelligence (OSINT), about an organization that could identify where or how machine learning is being used in a system, and help tailor an attack to make it more effective. These sources of information include technical publications, blog posts, press releases, software repositories, public data repositories, and social media postings.
  tactics:
    - org.mitre.attack.enterprise.TA0043  # Reconnaissance

- id: MLT0001
  object-type: technique
  name: ML Model Discovery
  description: |
    Adversaries may attempt to identify machine learning pipelines that exist on the system and gather information about them, including the software stack used to train and deploy models, training and testing data repositories, model repositories, and software repositories containing algorithms. This information can be used to identify targets for further collection, exfiltration, or disruption, or to tailor and improve attacks.
  tactics:
    - org.mitre.attack.enterprise.TA0043  # Reconnaissance

- id: MLT0002
  object-type: technique
  name: Reveal ML Ontology
  description: |
    By ML Ontology, we are referring to specific components of the ML system such as dataset (image, audio, tabular, NLP), features (handcrafted or learned), model / learning algorithm (gradient based or non-gradient based), parameters / weights. Depending on how much information is known, it can be a greybox or whitebox level of attacker knowledge.
  subtechnique-of: MLT0001  # ML Model Discovery
  tactics:
    - org.mitre.attack.enterprise.TA0043  # Reconnaissance

- id: MLT0003
  object-type: technique
  name: Reveal ML Model Family
  description: |
    Here the specifics of ML Models are not known and can generally be thought of as blackbox attacks. The attacker is able to only glean the model task, model input and model output. But because of the nature of the blog posts or papers that are published, some mention of the algorithms such as "Deep Learning" may squarely indicate that the underlying algorithm is gradient based.
  subtechnique-of: MLT0001  # ML Model Discovery
  tactics:
    - org.mitre.attack.enterprise.TA0043  # Reconnaissance

- id: MLT0004
  object-type: technique
  name: Gathering Datasets
  description: |
    Adversaries may collect datasets similar to those used by a particular organization or in a specific approach. Datasets may be identified when [Acquiring OSINT Information](#Acquire-OSINT-Information). This may allow the adversary to replicate a private model's functionality, constituting Intellectual Property Theft, or enable the adversary to carry out other attacks such as an [Evasion Attack](#Evasion-Attack).
  tactics:
    - org.mitre.attack.enterprise.TA0043  # Reconnaissance

- id: MLT0005
  object-type: technique
  name: Exploit Physical Environment
  description: |
    In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks. Recent work has show successful false positive and evasion attacks using physically printed patterns that are placed into scenes to disrupt and attack machine learning models. MITRE has recently created a dataset based on these [physically printed patterns](https://apricot.mitre.org/) to help researchers and practitioners better understand these attacks.
  tactics:
    - org.mitre.attack.enterprise.TA0043  # Reconnaissance

- id: MLT0006
  object-type: technique
  name: Model Replication
  description: |
    An adversary may replicate a model's functionality by training a shadow model by exploiting its API, or by leveraging pre-trained weights.
  tactics:
    - org.mitre.attack.enterprise.TA0043  # Reconnaissance

- id: MLT0007
  object-type: technique
  name: Exploit API - Shadow Model
  description: |
    An adversary may replicate a machine learning model's functionality by exploiting its inference API. In this case of model replication, the attacker repeatedly queries the victim's inference API and uses it as an oracle to collect combination of data and label. From the combination of (data,label), the attacker builds a shadow model, that effectively functions as the victim model -- but with lower fidelity. This is generally the first step in model evasion.
  subtechnique-of: MLT0006  # Model Replication
  tactics:
    - org.mitre.attack.enterprise.TA0043  # Reconnaissance

- id: MLT0008
  object-type: technique
  name: Alter Publicly Available, Pre-Trained Weights
  description: |
    An adversary uses pre-trained weights of one model to replicate a related model's functionality. For instance, researchers wanted to replicated GPT-2, a large language model. So, the researchers used the pre-trained weights of Grover, another NLP model, and modified it using GPT-2's objective function and training data, which effectively resulted in a shadow GPT-2 model (though with lower fidelity).
  subtechnique-of: MLT0006  # Model Replication
  tactics:
    - org.mitre.attack.enterprise.TA0043  # Reconnaissance

- id: MLT0009
  object-type: technique
  name: Model Stealing
  description: |
    Machine learning models' functionality can be stolen exploiting an inference API. There is a difference between Model Extraction and Model Replication: in model extraction attacks, the attacker is able to build a shadow model whose fidelity matches that of the victim model and hence, model stealing/extraction attacks lead to Stolen Intellectual Property. In [Model Replication](#Model-Replication) attacks, the shadow model does not have the same fidelity as that of the victim model.
  tactics:
    - org.mitre.attack.enterprise.TA0043  # Reconnaissance

- id: MLT0010
  object-type: technique
  name: Pre-Trained ML Model with Backdoor
  description: |
    Adversaries may gain initial access to a system by compromising portions of the ML supply chain. This could include GPU hardware, data and its annotations, parts of the ML software stack, or the model itself. In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.
  tactics:
    - org.mitre.attack.enterprise.TA0001  # Initial Access

- id: MLT0011
  object-type: technique
  name: Execute Unsafe ML Models
  description: |
    An Adversary may utilize unsafe ML Models that when executed have an unintended effect. The adversary can use this technique to establish persistent access to systems. These models may be introduced via a [Pre-Trained Model with Backdoor](#Pre-Trained-ML-Model-with-Backdoor).
  tactics:
    - org.mitre.attack.enterprise.TA0002  # Execution

- id: MLT0012
  object-type: technique
  name: ML Models from Compromised Sources
  description: |
    In Model Zoo such as "Caffe Model Zoo" or "ONNX Model Zoo" a collection of state of the art ML, pre-trained ML models are available so that ML engineers do not have to spend resources training ML models from scratch (hence "pre-trained"). An adversary may be able to compromise the model by checking in malicious code into the repository or perform a Man-in-the-Middle attack as the models are downloaded.
  subtechnique-of: MLT0011  # Execute Unsafe ML Models
  tactics:
    - org.mitre.attack.enterprise.TA0002  # Execution

- id: MLT0013
  object-type: technique
  name: Pickle Embedding
  description: |
    Python is one of the most commonly used ML language. Python pickles are used in serializing and de-serializing a Python object structures. ML models are sometimes stored as pickles and shared. An adversary may use pickle embedding to introduce malicious data payloads which may result in remote code execution.
  subtechnique-of: MLT0011  # Execute Unsafe ML Models
  tactics:
    - org.mitre.attack.enterprise.TA0002  # Execution

- id: MLT0014
  object-type: technique
  name: Execute unsafe ML Model Execution
  description: |
    An Adversary may utilize unsafe ML Models that when executed have an unintended effect. The adversary can use this technique to establish persistent access to systems. These models may be introduced via a [Pre-trained Model with Backdoor](#Pre-Trained-ML-Model-with-Backdoor). An example of this technique is to use pickle embedding to introduce malicious data payloads.
  tactics:
    - org.mitre.attack.enterprise.TA0003  # Persistence

- id: MLT0015
  object-type: technique
  name: Evasion Attack
  description: |
    Unlike poisoning attacks that needs access to training data, adversaries can fool an ML classifier by simply corrupting the query to the ML model. More broadly, the adversary can create data inputs that prevent a machine learning model from positively identifying the data sample. This technique can be used to evade an ML model to correctly classify it in the downstream task.
  tactics:
    - MLTA0000  # Model Evasion

- id: MLT0016
  object-type: technique
  name: Offline Evasion
  description: |
    In this case, the attacker has an offline copy of the ML model that was obtained via Model Replication or Model Extraction - depending on the case, the offline copy may be a shadow copy or a faithful reconstruction of the original model. While the goal of the adversary is to evade an online model, having access to an Offline model provides a space for the attacker to evade ML model without the fear of tripwires. Once the sample that evades the ML model is found, the attacker can essentially replay the sample to the victim, online model and be successful in the operation.\n
    Now this asks the question - how can an an adversary find the sample algorithmically that evades the offline ML model? There are many strategies at play, and depending on the economics, the attacker may choose one from the following: Simple Transformation of the input (cropping, shearing, translation), Common Corruption (adding white noise in the background), Adversarial Examples (carefully perturbing the input to achieve desired output) and Happy String (wherein the benign input is tacked onto malicious query points).
  subtechnique-of: MLT0015  # Evasion Attack
  tactics:
    - MLTA0000  # Model Evasion

- id: MLT0017
  object-type: technique
  name: Online Evasion
  description: |
    The same sub techniques like Simple Transformation, Common Corruption, Adversarial Examples, Happy Strings work also in the context of Online evasion attacks. The distinction between Offline and Online is if the model under attack is either stolen/replicated or if it is the live ML model.
  subtechnique-of: MLT0015  # Evasion Attack
  tactics:
    - MLTA0000  # Model Evasion

- id: MLT0018
  object-type: technique
  name: Model Poisoning
  description: |
    Adversaries can train machine learning models that are performant, but contain backdoors that produce inference errors when presented with input containing a trigger defined by the adversary. A model with a backdoor can be introduced by an innocent user via a [pre-trained model with backdoor](#Pre-Trained-ML-Model-with-Backdoor) or can be a result of [Data Poisoning](#Data-Poisoning). This backdoored model can be exploited at inference time with an [Evasion Attack](#Evasion-Attack).
  tactics:
    - MLTA0000  # Model Evasion

- id: MLT0019
  object-type: technique
  name: Data Poisoning
  description: |
    Adversaries may attempt to poison datasets used by a ML system by modifying the underlying data or its labels. This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable. The embedded vulnerability can be activated at a later time by providing the model with data containing the trigger. Data Poisoning can help enable attacks such as [ML Model Evasion](#Evasion-Attack).
  tactics:
    - MLTA0000  # Model Evasion

- id: MLT0020
  object-type: technique
  name: Tainting Data from Acquisition - Label Corruption
  description: |
    Adversaries may attempt to alter labels in a training set causing the model to misclassify.
  subtechnique-of: MLT0020  # Data Poisoning
  tactics:
    - MLTA0000  # Model Evasion

- id: MLT0021
  object-type: technique
  name: Tainting Data from Open Source Supply Chains
  description: |
    Adversaries may attempt to add their own data to an open source dataset which could create a classification backdoor.  For instance, the adversary could cause a targeted misclassification attack only when certain triggers are present in the query; and perform well otherwise.
  subtechnique-of: MLT0020  # Data Poisoning
  tactics:
    - MLTA0000  # Model Evasion

- id: MLT0022
  object-type: technique
  name: Tainting Data from Acquisition - Chaff Data
  description: |
    Adding noise to a dataset would lower the accuracy of the model, potentially making the model more vulnerable to misclassifications. For instance, researchers showed how they can overwhelm Splunk (and hence the ML models feeding from it), by simply adding potentially corrupted data. See [Attacking SIEM with Fake Logs](https://letsdefend.io/blog/attacking-siem-with-fake-logs/)
  subtechnique-of: MLT0020  # Data Poisoning
  tactics:
    - MLTA0000  # Model Evasion

- id: MLT0023
  object-type: technique
  name: Tainting Data in Training - Label Corruption
  description: |
    Changing training labels could create a backdoor in the model, such that a malicious input would always be classified to the benefit of the adversary. For instance, the adversary could cause a targeted misclassification attack only when certain triggers are present in the query; and perform well otherwise.
  subtechnique-of: MLT0020  # Data Poisoning
  tactics:
    - MLTA0000  # Model Evasion

- id: MLT0024
  object-type: technique
  name: Exfiltrate Training Data
  description: |
    Adversaries may exfiltrate private information related to machine learning models via their inference APIs. Additionally, adversaries can use these APIs to create copy-cat or proxy models.
  tactics:
    - org.mitre.attack.enterprise.TA0010  # Exfiltration

- id: MLT0025
  object-type: technique
  name: Membership Inference Attack
  description: |
    The membership of a data sample in a training set may be inferred by an adversary with access to an inference API. By simply querying the inference API of the victim model strategically -- and no extra access -- the adversary can cause privacy violations.
  subtechnique-of: MLT0024  # Exfiltrate Training Data
  tactics:
    - org.mitre.attack.enterprise.TA0010  # Exfiltration

- id: MLT0026
  object-type: technique
  name: ML Model Inversion
  description: |
    Machine learning models' training data could be reconstructed by exploiting the confidence scores that are available via an inference API. By simply querying the inference API strategically, an adversary could back out potentially private information  embedded within the training data. This could lead to privacy violations if the attacker can reconstruct the data of sensitive features used in the algorithm.
  subtechnique-of: MLT0024  # Exfiltrate Training Data
  tactics:
    - org.mitre.attack.enterprise.TA0010  # Exfiltration

- id: MLT0027
  object-type: technique
  name: ML Model Stealing
  description: |
    Machine learning models' functionality can be stolen exploiting an inference API. There is a difference between Model Extraction and Model Replication: in model extraction attacks, the attacker is able to build a shadow model whose fidelity matches that of the victim model and hence, model stealing/extraction attacks lead to [Stolen Intellectual Property](#Stolen-Intellectual-Property). In Model Replication attacks, shown above, the shadow model does not have the same fidelity as that of the victim model.
  tactics:
    - org.mitre.attack.enterprise.TA0010  # Exfiltration

- id: MLT0028
  object-type: technique
  name: Defacement
  description: |
    Adversaries can create data inputs that can be used to subvert the system for fun. This can be acheived corrupting the training data via poisoning as in the case of defacement of [Tay Bot](/pages/case-studies-page.md#tay-poisoning), Evasion or exploiting open CVEs in ML dev packages.
  tactics:
    - org.mitre.attack.enterprise.TA0040  # Impact

- id: MLT0029
  object-type: technique
  name: Denial of Service
  description: |
    Adversaries may target different Machine Learning services to conduct a DoS. One example of this type of attack is [Sponge examples](https://arxiv.org/abs/2006.03463) that could cause DoS on production NLP systems by wasting its energy consumption.
  tactics:
    - org.mitre.attack.enterprise.TA0040  # Impact
