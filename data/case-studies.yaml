---

- id: AML.CS0000
  name: Evasion of Deep Learning detector for malware C&C traffic
  object-type: case-study
  summary: |
    Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.
    Based on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.
    Then we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.
  incident-date: 2020-01-01
  dateGranularity: YEAR
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research_preprint.id}}"
      description: |
        We identified a machine learning based approach to malicious URL detection as a representative approach and potential target from the paper "URLNet: Learning a URL representation with deep learning for malicious URL detection" [1], which was found on arXiv (a pre-print repository).
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_data.id}}"
      description: |
        We acquired a similar dataset to the target production model.
    - tactic: *id_ml_attack_staging
      technique: "{{train_proxy_model.id}}"
      description: |
        We built a model that was trained on a similar dataset as the production model.
        We trained the model on ~ 33 million benign and ~ 27 million malicious HTTP packet headers.
        Evaluation showed a true positive rate of ~ 99% and false positive rate of ~0.01%, on average.
        Testing the model with a HTTP packet header from known malware command and control traffic samples was detected as malicious with high confidence (> 99%).
    - tactic: *id_ml_attack_staging
      technique: "{{craft_adv_manual.id}}"
      description: |
        We crafted evasion samples by removing fields from packet header which are typically not used for C&C communication (e.g. cache-control, connection, etc.)
    - tactic: *id_ml_attack_staging
      technique: "{{verify_attack.id}}"
      description: |
        We queried the model with our adversarial examples and adjusted them until the model was evaded.
    - tactic: *id_defense_evasion
      technique: "{{evade_model.id}}"
      description: |
        With the crafted samples we performed online evasion of the ML-based spyware detection model.
        The crafted packets were identified as benign with >80% confidence.
        This evaluation demonstrates that adversaries are able to bypass advanced ML detection techniques, by crafting samples that are misclassified by an ML model.
  reported-by: Palo Alto Networks (Network Security AI Research Team)
  references:
    - sourceDescription: 'Le, Hung, et al. "URLNet: Learning a URL representation with deep learning for malicious URL detection." arXiv preprint arXiv:1802.03162 (2018).'
      url:

- id: AML.CS0001
  name: Botnet Domain Generation Algorithm (DGA) Detection Evasion
  object-type: case-study
  summary: |
    The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.
    It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
    The generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.
  incident-date: 2020-01-01
  dateGranularity: YEAR
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research.id}}"
      description: |
        DGA detection is a widely used technique to detect botnets in academia and industry.
        The searched for research papers related to DGA detection.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts.id}}"
      description: |
        The researchers acquired a publicly available CNN-based DGA detection model [1] and tested against a well-known DGA generated domain name data sets, which includes ~50 million domain names from 64 botnet DGA families.
        The CNN-based DGA detection model shows more than 70% detection accuracy on 16 (~25%) botnet DGA families.
    - tactic: *id_resource_development
      technique: "{{develop_advml.id}}"
      description: |
        The researchers developed a generic mutation technique that requires a minimal number of iterations.
    - tactic: *id_ml_attack_staging
      technique: "{{craft_adv_blackbox.id}}"
      description: |
        The researchers used the mutation technique to generate evasive domain names.
    - tactic: *id_ml_attack_staging
      technique: "{{verify_attack.id}}"
      description: |
        Experiment results show that, after only one string is inserted once to the DGA generated domain names, the detection rate of all 16 botnet DGA families can drop to less than 25% detection accuracy.
    - tactic: *id_defense_evasion
      technique: "{{evade_model.id}}"
      description: |
        The DGA generated domain names mutated with this technique successfully evade the target DGA Detection model, allowing an adversary to continue communication with their [Command and Control]() servers.
  reported-by: Palo Alto Networks (Network Security AI Research Team)
  references:
    - sourceDescription: '[1] Yu, Bin, Jie Pan, Jiaming Hu, Anderson Nascimento, and Martine De Cock. "Character level based detection of DGA domain names." In 2018 International Joint Conference on Neural Networks (IJCNN), pp. 1-8. IEEE, 2018. Source code is available from Github: https://github.com/matthoffman/degas'
      url: https://github.com/matthoffman/degas

- id: AML.CS0002
  name: VirusTotal Poisoning
  object-type: case-study
  summary: |
    An increase in reports of a certain ransomware family that was out of the ordinary was noticed.
    In investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.
    Further investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.
    Interestingly enough, the compile time was the same for all the samples.
    After more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.
    The variants wouldn't always be executable but still classified as the same ransomware family.
  incident-date: 2020-01-01
  dateGranularity: YEAR
  procedure:
    - tactic: *id_resource_development
      technique: "{{obtain_advml.id}}"
      description: |
        The actor obtained [metame](https://github.com/a0rtega/metame), a simple metamorphic code engine for arbitrary executables.
    - tactic: *id_ml_attack_staging
      technique: "{{craft_adv.id}}"
      description: |
        The actor used a malware sample from a prevalent ransomware family as a start to create 'mutant' variants.
    - tactic: *id_initial_access
      technique: "{{supply_chain_data.id}}"
      description: |
        The actor uploaded "mutant" samples to the platform.
    - tactic: *id_persistence
      technique: "{{poison_data.id}}"
      description: |
        Several vendors started to classify the files as the ransomware family even though most of them won't run.
        The "mutant" samples poisoned the dataset the ML model(s) use to identify and classify this ransomware family.
  reported-by: Christiaan Beek (@ChristiaanBeek) - McAfee Advanced Threat Research
  references:

- id: AML.CS0003
  name: Bypassing Cylance's AI Malware Detection
  object-type: case-study
  summary: |
    Researchers at Skylight were able to create a universal bypass string that
    when appended to a malicious file evades detection by Cylance's AI Malware detector.
  incident-date: 2019-09-07
  dateGranularity: DATE
  procedure:
    - tactic: *id_reconnaissance
      technique: T1594  # Search Victim-Owned Websites
      description: |
        The researchers read publicly available information about Cylance's AI Malware detector.
    - tactic: *id_ml_model_access
      technique: "{{ml_service.id}}"
      description: |
        The researchers used Cylance's AI Malware detector and enabled verbose logging to understand the inner workings of the ML model, particularly around reputation scoring.
    - tactic: *id_resource_development
      technique: "{{develop_advml.id}}"
      description: |
        The researchers used the reputation scoring information to reverse engineer which attributes provided what level of positive or negative reputation.
        Along the way, they discovered a secondary model which was an override for the first model.
        Positive assessments from the second model overrode the decision of the core ML model.
    - tactic: *id_ml_attack_staging
      technique: "{{craft_adv_manual.id}}"
      description: |
        Using this knowledge, the researchers fused attributes of known good files with malware to manually create adversarial malware.
    - tactic: *id_defense_evasion
      technique: "{{evade_model.id}}"
      description: |
        Due to the secondary model overriding the primary, the researchers were effectively able to bypass the ML model.
  reported-by: Research and work by Adi Ashkenazy, Shahar Zini, and SkyLight Cyber team. Notified to us by Ken Luu (@devianz_)
  references:
    - sourceDescription:
      url: https://skylightcyber.com/2019/07/18/cylance-i-kill-you/

- id: AML.CS0004
  name: Camera Hijack Attack on Facial Recognition System
  object-type: case-study
  summary: |
    This type of attack can break through the traditional live detection model
    and cause the misuse of face recognition.
  incident-date: 2020-01-01
  dateGranularity: YEAR
  procedure:
    - tactic: *id_resource_development
      technique: T1583  # Acquire Infrastructure
      description: |
        The attackers bought customized low-end mobile phones.
    - tactic: *id_resource_development
      technique: T1588.002  # Obtain Capabilities: Tool
      description: |
        The attackers obtained customized android ROMs and a virtual camera application.
    - tactic: *id_resource_development
      technique: "{{obtain_advml.id}}"
      description: |
        The attackers obtained software that turns static photos into videos, adding realistic effects such as blinking eyes.
    - tactic: *id_collection
      technique: T1213  #  Data from Information Repositories
      description: |
        The attackers collected user identity information and face photos.
    - tactic: *id_resource_development
      technique: T1585  # Establish Accounts
      description: |
        The attackers registered accounts with the victims' identity information.
    - tactic: *id_ml_model_access
      technique: "{{ml_service.id}}"
      description: |
        The attackers used the virtual camera app to present the generated video to the ML-based facial recongition product used for user verification.
    - tactic: *id_impact
      technique: "{{evade_model.id}}"
      description: |
        The attackers successfully evaded the face recognition system and impersonated the victim.
  reported-by: Henry Xuef, Ant Group AISEC Team
  references:

- id: AML.CS0005
  name: Attack on Machine Translation Service - Google Translate, Bing Translator, and Systran Translate
  object-type: case-study
  summary: |
    Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
    A research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.
    Beyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
    These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.
  incident-date: 2020-04-30
  dateGranularity: DATE
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research.id}}"
      description: |
        The researchers used published research papers to identify the datasets and model architectures used by the target translaation services.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_data.id}}"
      description: |
        The researchers gathered similar datasets that the target translation services used.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_model.id}}"
      description: |
        The researchers gathered similar model architectures that the target translation services used.
    - tactic: *id_ml_model_access
      technique: "{{inference_api.id}}"
      description: |
        They abuse a public facing application to query the model and produce machine translated sentence pairs as training data.
    - tactic: *id_ml_attack_staging
      technique: "{{replicate_model.id}}"
      description: |
        Using these translated sentence pairs, the researchers trained a model that replicates the behavior of the target model.
    - tactic: *id_impact
      technique: "{{ip_theft.id}}"
      description: |
        By replicating the model with high fidelity, the researchers demonstrated that an adversary could steal a model and violate the victim's intellectual property rights.
    - tactic: *id_ml_attack_staging
      technique: "{{craft_adv_transfer.id}}"
      description: |
        The replicated models were used to generate adversarial examples that successfully transferred to the black-box translation services.
    - tactic: *id_impact
      technique: "{{evade_model.id}}"
      description: |
        The adversarial examples were used to evade the machine translation services.
  reported-by: Work by Eric Wallace, Mitchell Stern, Dawn Song and reported by Kenny Song (@helloksong)
  references:
    - sourceDescription:
      url: https://arxiv.org/abs/2004.15015
    - sourceDescription:
      url: https://www.ericswallace.com/imitation

- id: AML.CS0006
  name: ClearviewAI Misconfiguration
  object-type: case-study
  summary: |
    Clearview AI's source code repository, though password protected, was misconfigured to allow an arbitrary user to register an account.
    This allowed an external researcher to gain access to a private code repository that contained Clearview AI production credentials, keys to cloud storage buckets containing 70K video samples, and copies of its applications and Slack tokens.
    With access to training data, a bad-actor has the ability to cause an arbitrary misclassification in the deployed model.
    These kinds of attacks illustrate that any attempt to secure ML system should be on top of "traditional" good cybersecurity hygiene such as locking down the system with least privileges, multi-factor authentication and monitoring and auditing.
  incident-date: 2020-04-16
  dateGranularity: DATE
  procedure:
    - tactic: *id_initial_access
      technique: T1078  # Valid Accounts
      description: |
        In this scenario, a security researcher gained initial access to via a valid account that was created through a misconfiguration.
  reported-by: Mossab Hussein (@mossab_hussein)
  references:
    - sourceDescription:
      url: https://techcrunch.com/2020/04/16/clearview-source-code-lapse/amp/
    - sourceDescription:
      url: https://gizmodo.com/we-found-clearview-ais-shady-face-recognition-app-1841961772

- id: AML.CS0007
  name: GPT-2 Model Replication
  object-type: case-study
  summary: |
    OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model. Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.
  incident-date: 2019-08-22
  dateGranularity: DATE
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research.id}}"
      description: |
        Using the public documentation about GPT-2, ML researchers gathered information about the dataset, model architecture, and training hyper-parameters.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_model.id}}"
      description: |
        The researchers obtained a reference implementation of a similar publicly available model called Grover.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_data.id}}"
      description: |
        The researchers were able to manually recreate the dataset used in the original GPT-2 paper using the gathered documentation.
    - tactic: *id_resource_development
      technique: "{{acquire_workspaces.id}}"
      description: |
        The researchers were able to use TensorFlow Research Cloud via their academic credentials.
    - tactic: *id_ml_attack_staging
      technique: "{{train_proxy_model.id}}"
      description: |
        The researchers modified Grover's objective function to reflect GPT-2's objective function and then trained on the dataset they curated.
        They used Grover's initial hyperparameters for training.
        This resulted in their replicated model.
  reported-by: Vanya Cohen (@VanyaCohen), Aaron Gokaslan (@SkyLi0n), Ellie Pavlick, Stefanie Tellex
  references:
    - sourceDescription:
      url: https://www.wired.com/story/dangerous-ai-open-source/
    - sourceDescription:
      url: https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc

- id: AML.CS0008
  name: ProofPoint Evasion
  object-type: case-study
  summary: |
    CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.
  incident-date: 2019-09-09
  dateGranularity: DATE
  procedure:
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts.id}}"
      description: |
        The researchers first gathered the scores from the Proofpoint's ML system used in email headers by sending a large number of emails through the system and scraping the model scores exposed in the logs.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_data.id}}"
      description: |
        The researchers converted the collected scores into a dataset.
    - tactic: *id_ml_attack_staging
      technique: "{{train_proxy_model.id}}"
      description: |
        Using these scores, the researchers replicated the ML mode by building a "shadow" aka copy-cat ML model.
    - tactic: *id_ml_attack_staging
      technique: "{{craft_adv_whitebox.id}}"
      description: |
        Next, the ML researchers algorithmically found samples that this "offline" copy cat model.
    - tactic: *id_ml_attack_staging
      technique: "{{craft_adv_transfer.id}}"
      description: |
        Finally, these insights from the offline model allowed the researchers to create malicious emails that received preferable scores from the real ProofPoint email protection system, hence bypassing it.
  reported-by: Will Pearce (@moo_hax), Nick Landers (@monoxgas)
  references:
    - sourceDescription:
      url: https://nvd.nist.gov/vuln/detail/CVE-2019-20634
    - sourceDescription:
      url: https://github.com/moohax/Talks/blob/master/slides/DerbyCon19.pdf
    - sourceDescription:
      url: https://github.com/moohax/Proof-Pudding

- id: AML.CS0009
  name: Tay Poisoning
  object-type: case-study
  summary: |
    Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes. Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.
  incident-date: 2016-03-23
  dateGranularity: DATE
  procedure:
    - tactic: *id_ml_model_access
      technique: "{{inference_api.id}}"
      description: |
        Adversaries were able to interact with Tay via a few different publicly available methods.
    - tactic: *id_initial_access
      technique: "{{supply_chain_data.id}}"
      description: |
        Tay bot used the interactions with its twitter users as training data to improve its conversations.
        Adversaries were able to coordinate with the intent of defacing Tay bot by exploiting this feedback loop.
    - tactic: *id_persistence
      technique: "{{poison_data.id}}"
      description: |
        By repeatedly interacting with Tay using racist and offensive language, they were able to bias Tay's dataset towards that language as well.
    - tactic: *id_impact
      technique: "{{erode_integrity.id}}"
      description: |
        As a result of this coordinated attack, Tay's conversation algorithms began to learn to generate reprehensible material.
        This quickly lead to its decommissioning.
  reported-by: Microsoft
  references:
    - sourceDescription:
      url: https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/
    - sourceDescription:
      url: https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation

- id: AML.CS0010
  name: Microsoft - Azure Service
  object-type: case-study
  summary:
    The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service.
    This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.
  incident-date: 2020-01-01
  dateGranularity: YEAR
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research.id}}"
      description: |
        The team first performed reconnaissance to gather information about the target ML model.
    - tactic: *id_initial_access
      technique: T1078
      description: |
        The team used a valid account to gain access to the network.
    - tactic: *id_collection
      technique: "{{ml_artifact_collection.id}}"
      description: |
        The team found the model file of the target ML model and the necessary training data.
    - tactic: *id_ml_attack_staging
      technique: "{{craft_adv_whitebox.id}}"
      description: |
        Using the target model and data, the red team crafted evasive adversarial data.
    - tactic: *id_ml_model_access
      technique: "{{inference_api.id}}"
      description: |
        The team used an exposed API to access the target model.
    - tactic: *id_impact
      technique: "{{evade_model.id}}"
      description: |
        The team performed an online evasion attack by replaying the adversarial examples, which helped achieve this goal.
  reported-by: Microsoft (Azure Trustworthy Machine Learning)
  references:

- id: AML.CS0011
  name: Microsoft Edge AI - Evasion
  object-type: case-study
  summary: |
    The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.
  incident-date: 2020-02-01
  dateGranularity: MONTH
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research.id}}"
      description: |
        The team first performed reconnaissance to gather information about the target ML model.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts.id}}"
      description: |
        The team identified and obtained the publicly available base model.
    - tactic: *id_ml_model_access
      technique: "{{inference_api.id}}"
      description: |
        Then using the publicly available version of the ML model, started sending queries and analyzing the responses (inferences) from the ML model.
    - tactic: *id_ml_attack_staging
      technique: "{{craft_adv_blackbox.id}}"
      description: |
        The red team created an automated system that continuously manipulated an original target image, that tricked the ML model into producing incorrect inferences, but the perturbations in the image were unnoticeable to the human eye.
    - tactic: *id_impact
      technique: "{{evade_model.id}}"
      description: |
        Feeding this perturbed image, the red team was able to evade the ML model by causing misclassifications.
  reported-by: Microsoft
  references:

- id: AML.CS0012
  name: MITRE - Physical Adversarial Attack on Face Identification
  object-type: case-study
  summary: |
   MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
   This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.
  incident-date: 2020-01-01
  dateGranularity: YEAR
  procedure:
    - tactic: *id_reconnaissance
      technique: "{{victim_research.id}}"
      description: |
        The team first performed reconnaissance to gather information about the target ML model.
    - tactic: *id_initial_access
      technique: T1078
      description: |
        The team gained access via a valid account.
    - tactic: *id_ml_model_access
      technique: "{{inference_api.id}}"
      description: |
        The team accessed the inference API of the target model.
    - tactic: *id_discovery
      technique: "{{discover_model_ontology.id}}"
      description: |
        The team identified the list of identities targeted by the model by querying the target model's inference API.
    - tactic: *id_resource_development
      technique: "{{acquire_ml_artifacts_data.id}}"
      description: |
        The team acquired representative open source data.
    - tactic: *id_ml_attack_staging
      technique: "{{train_proxy_model.id}}"
      description: |
        The team developed a proxy model using the open source data.
    - tactic: *id_ml_attack_staging
      technique: "{{craft_adv_whitebox.id}}"
      description: |
        Using the proxy model, the red team optimized a physical domain patch-based attack using expectation over transformation.
    - tactic: *id_ml_model_access
      technique: "{{physical_env.id}}"
      description: |
        The team placed the physical countermeasure in the physical environment.
    - tactic: *id_impact
      technique: "{{evade_model.id}}"
      description: |
        The team successfully evaded the model using the physical countermeasure and causing targeted misclassifications.
  reported-by: MITRE AI Red Team
  references:

- id: AML.CS0013
  name: Backdoor Attack on Deep Learning Models in Mobile Apps
  object-type: case-study
  summary: >
    Deep learning models are increasingly used in mobile applications as
    critical components. Researchers from Microsoft Research demonstrated that
    many deep learning models deployed in mobile apps are vulnerable to backdoor
    attacks via "neural payload injection." They conducted an empirical study on
    real-world mobile deep learning apps collected from Google Play, and found
    54 apps that were vulnerable to attack, including popular security and
    safety critical applications used for as cash recognition, parental control,
    face authentication, and financial services among others.
  incident-date: 2021-01-18T00:00:00.000Z
  incident-date-granularity: DATE
  procedure:
    - tactic: TA0043
      technique: T1593.002
      description: >+
        To identify a list of potential target models, the researchers searched
        the Google Play store for apps that may contain embedded deep learning
        models by searching for deep learning related keywords.

    - tactic: AML.TA0000
      technique: AML.T0044
      description: >+
        The researchers acquired the apps' APKs from the Google Play store.

        They filtered the list of potential target applications by searching the
        code metadata for keywords related to TensorFlow or TFLite and their
        model binary formats (.tf and .tflite).

        The models were extracted from the APKs using Apktool.

        This provided the researches with full access to the ML model, albeit in
        compiled, binary form.

    - tactic: TA0042
      technique: AML.T0017
      description: >+
        The researchers developed a novel approach to insert a backdoor into a
        compiled model that can be activated with a visual trigger.

        They inject a "neural payload" into the model that consists of a trigger
        detection network and conditional logic.

        The trigger detector is trained to detect a visual trigger that will be
        placed in the real world.

        The conditional logic allows the researchers to bypass the victim model
        when the trigger is detected and provide model outputs of their
        choosing.


        The only requirements for training a trigger detector are a general
        dataset from the same modality as the target model (e.g.

        ImageNet for image classification) and several photos of the desired
        trigger.

    - tactic: TA0003
      technique: AML.T0018
      description: >+
        The researchers poisoned the victim model by injecting the neural
        payload into the compiled models by directly modifying the computation
        graph.

        The researchers then repackage the poisoned model back into the APK

    - tactic: AML.TA0001
      technique: AML.T0042
      description: >+
        To verify the success of the attack, the researchers confirmed the app
        did not crash with the malicious model in place, and that the trigger
        detector successfully detects the trigger.

    - tactic: TA0001
      technique: AML.T0010.003
      description: >+
        In practice, the malicious APK would need to be installed on victim's
        devices via a supply chain compromise.

    - tactic: AML.TA0001
      technique: AML.T0043.004
      description: >+
        The trigger is placed in the physical environment, where it is captured
        by the victim's device camera and processed by the backdoored ML model.

    - tactic: AML.TA0000
      technique: AML.T0041
      description: >+
        At inference time, only physical environment access is required to
        trigger the attack.

    - tactic: TA0040
      technique: AML.T0015
      description: >+
        Presenting the visual trigger causes the victim model to be bypassed.

        The researchers demonstrated this can be used to evade ML models in
        several safety-critical apps in the Google Play store.

  reported-by:
    - >-
      Neil Yale / YingZonghao (Organization:University of Chinese Academy of
      Sciences)
  references:
    - sourceDescription: >-
        DeepPayload: Black-box Backdoor Attack on Deep Learning Models through
        Neural Payload Injection
      url: https://arxiv.org/abs/2101.06896

- id: AML.CS0014
  name: Kaspersky - Confusing Antimalware Neural Networks
  object-type: case-study
  summary: >
    Cloud storage and computations have become popular platforms for deploying
    ML malware detectors. In such cases, the features for models are built on
    users' systems and then sent to cybersecurity company servers. The Kaspersky
    ML research team explored this gray-box scenario and shown that feature
    knowledge is enough for an adversarial attack on ML models.

    They attacked one of Kaspersky's antimalware ML models without white-box
    access to it and successfully evaded detection for most of the adversarially
    modified malware files.
  incident-date: 2021-06-23T00:00:00.000Z
  incident-date-granularity: DATE
  procedure:
    - tactic: TA0043
      technique: AML.T0001
      description: >+
        The researchers performed a review of adversarial ML attacks on
        antimalware products.

        They discovered that techniques borrowed from attacks on image
        classifiers have been successfully applied to the antimalware domain.

        However, it was not clear if these approaches were effective against the
        ML component of production antimalware solutions.

    - tactic: TA0043
      technique: T1594
      description: >+
        Kaspersky's use of ML-based antimalware detectors is publicly documented
        on their website.

        In practice, an adversary could use this for targeting.

    - tactic: AML.TA0000
      technique: AML.T0047
      description: >+
        The researches used access to the target ML-based antimalware product
        throughout this case study.

        This product scans files on the user's system, extracts features
        locally, then sends them to the cloud-based ML malware detector for
        classification.

        Therefore, the researchers had only black-box access to the malware
        detector itself, but could learn valuable information for constructing
        the attack from the feature extractor.

    - tactic: TA0042
      technique: AML.T0002.000
      description: >+
        The researchers collected a dataset of malware and clean files.

        They scanned the dataset with the target ML-based antimalware solution
        and labeled the samples according the ML detector’s predictions.

    - tactic: AML.TA0001
      technique: AML.T0005
      description: >+
        Then, a proxy model was trained on the labeled dataset of malware and
        clean files.

        The researchers experimented with a variety of model architectures.

    - tactic: TA0042
      technique: AML.T0017
      description: >+
        By reverse engineering the local feature extractor, the researchers
        could collect information about the input features, used for the
        cloud-based ML detector.

        The model collects PE Header features, section features and section data
        statistics, and file strings information.

        A gradient based adversarial algorithm for executable files was
        developed.

        The algorithm manipulates file features to avoid detection by the proxy
        model, while still containing the same malware payload

    - tactic: AML.TA0001
      technique: AML.T0043.002
      description: >+
        Using a developed gradient-driven algorithm, malicious adversarial files
        for the proxy model were constructed from the malware files for
        black-box transfer to the target model.

    - tactic: AML.TA0001
      technique: AML.T0042
      description: >+
        The adversarial malware files were tested against the target antimalware
        solution to verify their efficacy.

    - tactic: TA0005
      technique: AML.T0015
      description: >+
        The researchers demonstrated that for most of the adversarial files, the
        antimalware model was successfully evaded.

        In practice, an adversary could deploy their adversarially crafted
        malware and infect systems while evading detection.

  reported-by: 'Alexey Antonov and Alexey Kogtenkov (ML researchers, Kaspersky ML team) '
  references:
    - sourceDescription: >-
        How to confuse antimalware neural networks. Adversarial attacks and
        protection
      url: >-
        https://securelist.com/how-to-confuse-antimalware-neural-networks-adversarial-attacks-and-protection/102949/
