{
  "tactic": {
    "id": "AML.TA0011",
    "name": "Impact",
    "description": "The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your AI systems and data.\n\nImpact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.\nTechniques used for impact can include destroying or tampering with data.\nIn some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.\nThese techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.",
    "object-type": "tactic",
    "created_date": "2022-01-24",
    "modified_date": "2025-04-09",
    "ATT&CK-reference": {
      "id": "TA0040",
      "url": "https://attack.mitre.org/tactics/TA0040/"
    }
  },
  "summary": {
    "total_techniques": 7,
    "main_techniques": 7,
    "subtechniques": 0
  },
  "techniques": {
    "main_techniques": [
      {
        "id": "AML.T0015",
        "name": "Evade AI Model",
        "description": "Adversaries can [Craft Adversarial Data](/techniques/AML.T0043) that prevent a AI model from correctly identifying the contents of the data.\nThis technique can be used to evade a downstream task where AI is utilized.\nThe adversary may evade AI based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.",
        "object-type": "technique",
        "tactics": [
          "AML.TA0004",
          "AML.TA0007",
          "AML.TA0011"
        ],
        "created_date": "2021-05-13",
        "modified_date": "2025-04-09"
      },
      {
        "id": "AML.T0029",
        "name": "Denial of AI Service",
        "description": "Adversaries may target AI-enabled systems with a flood of requests for the purpose of degrading or shutting down the service.\nSince many AI systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded.\nAdversaries can intentionally craft inputs that require heavy amounts of useless compute from the AI system.",
        "object-type": "technique",
        "tactics": [
          "AML.TA0011"
        ],
        "created_date": "2021-05-13",
        "modified_date": "2025-04-09"
      },
      {
        "id": "AML.T0031",
        "name": "Erode AI Model Integrity",
        "description": "Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time.\nThis can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.\n",
        "object-type": "technique",
        "tactics": [
          "AML.TA0011"
        ],
        "created_date": "2021-05-13",
        "modified_date": "2025-04-09"
      },
      {
        "id": "AML.T0034",
        "name": "Cost Harvesting",
        "description": "Adversaries may target different AI services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization.\nSponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.",
        "object-type": "technique",
        "tactics": [
          "AML.TA0011"
        ],
        "created_date": "2021-05-13",
        "modified_date": "2025-04-09"
      },
      {
        "id": "AML.T0046",
        "name": "Spamming AI System with Chaff Data",
        "description": "Adversaries may spam the AI system with chaff data that causes increase in the number of detections.\nThis can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.",
        "object-type": "technique",
        "tactics": [
          "AML.TA0011"
        ],
        "created_date": "2021-05-13",
        "modified_date": "2025-04-09"
      },
      {
        "id": "AML.T0048",
        "name": "External Harms",
        "description": "Adversaries may abuse their access to a victim system and use its resources or capabilities to further their goals by causing harms external to that system.\nThese harms could affect the organization (e.g. Financial Harm, Reputational Harm), its users (e.g. User Harm), or the general public (e.g. Societal Harm).\n",
        "object-type": "technique",
        "tactics": [
          "AML.TA0011"
        ],
        "created_date": "2022-10-27",
        "modified_date": "2023-10-25"
      },
      {
        "id": "AML.T0059",
        "name": "Erode Dataset Integrity",
        "description": "Adversaries may poison or manipulate portions of a dataset to reduce its usefulness, reduce trust, and cause users to waste resources correcting errors.",
        "object-type": "technique",
        "tactics": [
          "AML.TA0011"
        ],
        "created_date": "2025-03-12",
        "modified_date": "2025-03-12"
      }
    ],
    "subtechniques": []
  }
}